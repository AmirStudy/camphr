{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bedoner.lang.juman import Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.strings import StringStore\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "with open(\"../bedore-nlp-modules/data/Japanese_L-12_H-768_A-12_E-30_BPE/vocab.txt\") as f:\n",
    "    vs = []\n",
    "    for line in f:\n",
    "        vs.append(line[:-1])\n",
    "\n",
    "s=StringStore(vs)\n",
    "\n",
    "v=Vocab(strings=s)\n",
    "v.to_disk(\"data/vocab.Japanese_L-12_H-768_A-12_E-30_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mojimoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_pytorch_transformers.pipeline.wordpiecer import PyTT_WordPiecer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_pytorch_transformers._tokenizers import SerializableBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Japanese(v)\n",
    "w = PyTT_WordPiecer(v)\n",
    "wp = SerializableBertTokenizer(\"../bedore-nlp-modules/data/Japanese_L-12_H-768_A-12_E-30_BPE/vocab.txt\", do_lower_case=False, tokenize_chinese_chars=False)\n",
    "w.model = wp\n",
    "nlp.add_pipe(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"EXILEのATSUSHIと中島美嘉が14日ニューヨーク入り\"\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(\n",
    "    bert_config, num_labels, init_checkpoint, use_one_hot_embeddings, max_seq_length\n",
    "):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "        (total_loss, per_example_loss, logits, predicts) = create_model(\n",
    "            bert_config,\n",
    "            is_training,\n",
    "            input_ids,\n",
    "            input_mask,\n",
    "            segment_ids,\n",
    "            label_ids,\n",
    "            num_labels,\n",
    "            use_one_hot_embeddings,\n",
    "            max_seq_length,\n",
    "        )\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        # scaffold_fn = None\n",
    "        if init_checkpoint:\n",
    "            (\n",
    "                assignment_map,\n",
    "                initialized_variable_names,\n",
    "            ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode, predictions=predicts  # {\"probabilities\": probabilities},\n",
    "            )\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedore_nlp_modules.bert_modules import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "__dir__ = Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_pytorch_transformers._tokenizers.SerializableBertTokenizer at 0x132bfae10>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_pytorch_transformers._tokenizers.SerializableBertTokenizer at 0x132bfae10>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytt_wordpiecer']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(str(__dir__/\"data/Japanese_L-12_H-768_A-12_E-30_BPE/bert_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(__dir__ / \"data/bert_result_ene_0/label2id.pkl\"), \"rb\") as f:\n",
    "    label2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(label2id) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dir = __dir__ / \"data/Japanese_L-12_H-768_A-12_E-30_BPE/\"\n",
    "model_dir = __dir__ / \"data/bert_result_ene_0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_checkpoint = str(bert_dir/ \"bert_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=128\n",
    "predict_batch_size=8\n",
    "drop_remainder=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    num_labels=num_labels,\n",
    "    init_checkpoint=init_checkpoint,\n",
    "    use_one_hot_embeddings=None,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/bert_result_ene_0/id2label.json\", \"w\") as f:\n",
    "    json.dump(id2label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn, config=run_config, params={\"batch_size\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(a, l, c= 0):\n",
    "    if len(a) >= l:\n",
    "        return a[:l]\n",
    "    return a + [c]*(l - len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"input_ids\"] = np.array([pad(doc._.pytt_word_pieces, max_seq_length) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"input_mask\"] = np.array([pad([1 for _ in range(len(doc._.pytt_word_pieces))], max_seq_length) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = []\n",
    "for doc in docs:\n",
    "    seg = []\n",
    "    for i,s in enumerate(doc._.pytt_segments):\n",
    "        seg += [i] * len(s._.pytt_word_pieces)\n",
    "    segs.append(pad(seg, max_seq_length))\n",
    "features[\"segment_ids\"] = np.array(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3, 4, 5], [6], [7], [8, 9], [10], [11], [12], [13], [14]]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.pytt_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    label_id = [label2id[\"[CLS]\"]]\n",
    "    O,X = label2id[\"O\"], label2id[\"X\"]\n",
    "    for a in doc._.pytt_alignment:\n",
    "        label_id + [O] + [X] * (len(a)-1)\n",
    "    label_ids.append(pad(label_id, max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"label_ids\"] = np.array(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=tf.estimator.inputs.numpy_input_fn(features, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res = list(estimator.predict(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(id2label[r] for r in _res[0] if r != 0f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/bert_result_ene_0/id2label.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2label,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([406, 334, 404, 267, 405, 405, 404, 267, 266, 405, 404,  78,  79,\n",
       "         46, 404, 407,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ents = []\n",
    "cur = []\n",
    "for i,a in enumerate(doc._.pytt_alignment):\n",
    "    l = res[a[0]]\n",
    "    doc[i].ent_type_ = l\n",
    "    if l.startswith(\"B-\"):\n",
    "        if cur:\n",
    "            new_ents.append(Span(doc,cur[0], cur[1], cur[2]))\n",
    "        cur = [i,i+1,l]\n",
    "    elif l.startswith(\"I-\"):\n",
    "        cur[1]+=1\n",
    "if cur:\n",
    "    new_ents.append(Span(doc,cur[0], cur[1], cur[2]))\n",
    "doc.ents = new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents=[]\n",
    "for e in doc.ents:\n",
    "    ents.append((e.text, e.label_[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ＥＸＩＬＥ', 'Show_Organization'),\n",
       " ('ＡＴＳＵＳＨＩ', 'Person'),\n",
       " ('中島美嘉', 'Person'),\n",
       " ('１４日', 'Date'),\n",
       " ('ニューヨーク', 'City')]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_pytorch_transformers._tokenizers.SerializableBertTokenizer at 0x132bfae10>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.get_pipe(\"pytt_wordpiecer\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
