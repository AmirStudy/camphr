{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytest\n",
    "from bedoner.entity_extractors.bert_ner import BertEntityExtractor, create_estimator\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from spacy.tokens import Doc\n",
    "from bedoner.lang.juman import Japanese as Juman\n",
    "from spacy.strings import StringStore\n",
    "from spacy.vocab import Vocab\n",
    "from spacy_pytorch_transformers.pipeline.wordpiecer import PyTT_WordPiecer\n",
    "from bedoner.lang.mecab import Japanese\n",
    "from bedoner.wordpiecer import BertWordPiecer\n",
    "import json\n",
    "from pathlib import Path \n",
    "import shutil\n",
    "from spacy.cli import package\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"bert_ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_package(nlp):\n",
    "    meta=nlp.meta\n",
    "    req=meta.get(\"requirements\") or []\n",
    "    req.append(\"bedoner @ git+https://github.com/PKSHATechnology/bedore-ner\")\n",
    "    nlp.meta[\"requirements\"] = req\n",
    "    \n",
    "    pkgs = Path(\"../pkgs\")\n",
    "    with tempfile.TemporaryDirectory() as tmpd:\n",
    "        nlp.to_disk(str(tmpd))\n",
    "        package(tmpd, pkgs, force=True)\n",
    "    model_name  =  meta[\"lang\"] + \"_\" + meta[\"name\"]\n",
    "    pkgd = pkgs / (model_name+ \"-\" + meta[\"version\"])\n",
    "    return pkgd, tmpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__dir__ = Path(\".\").parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabfile = __dir__ / \"../data/Japanese_L-12_H-768_A-12_E-30_BPE/vocab.txt\"\n",
    "with vocabfile.open() as f:\n",
    "    vs = []\n",
    "    for line in f:\n",
    "        vs.append(line[:-1])\n",
    "s = StringStore(vs)\n",
    "v = Vocab(strings=s)\n",
    "nlp = Juman(v, meta={\"name\": name})\n",
    "w = BertWordPiecer(\n",
    "    v,\n",
    "    vocab_file=str(vocabfile)\n",
    ")\n",
    "w.model = w.Model(w.cfg[\"vocab_file\"])\n",
    "nlp.add_pipe(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dir = __dir__ / \"../data/Japanese_L-12_H-768_A-12_E-30_BPE\"\n",
    "model_dir = __dir__ / \"../data/bert_result_ene_0/\"\n",
    "init_checkpoint = str(bert_dir / \"bert_model.ckpt\")\n",
    "with (model_dir / \"label2id.json\").open(\"r\") as f:\n",
    "    label2id = json.load(f)\n",
    "\n",
    "bert_cfg = dict(\n",
    "    bert_dir=str(bert_dir),\n",
    "    model_dir=str(model_dir),\n",
    "    num_labels=len(label2id) + 1,\n",
    "    init_checkpoint=init_checkpoint,\n",
    "    use_one_hot_embeddings=None,\n",
    "    max_seq_length=128,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "ee = BertEntityExtractor.from_nlp(nlp, label2id=label2id, **bert_cfg)\n",
    "ee.model = create_estimator(**bert_cfg)\n",
    "ee.set_values()\n",
    "ee.create_predictor()\n",
    "nlp.add_pipe(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"EXILEのATSUSHIと中島美嘉が14日ニューヨーク入り\") \n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkgd, tmpd = create_package(nlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
